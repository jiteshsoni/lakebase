# Lakebase - Async Database Benchmarking & Connection Management

A comprehensive toolkit for benchmarking and managing async database connections with Databricks Lakebase, featuring secure credential management and high-performance connection pooling.

## ğŸš€ Features

- **Async Database Connections** - High-performance async connections with SQLAlchemy
- **Secure Credential Management** - Environment variables instead of plain JSON files
- **Connection Pooling** - Production-grade connection pooling with health monitoring
- **Benchmarking Tools** - Comprehensive performance testing with 100M+ row datasets
- **FastAPI Integration** - Ready-to-use FastAPI database modules
- **OAuth Token Management** - Automatic background token refresh for Lakebase

## ğŸ“ Project Structure

```
lakebase/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ requirements.txt                    # Dependencies
â”œâ”€â”€ .gitignore                         # Git ignore rules
â”œâ”€â”€ .env                               # Secure credentials (gitignored)
â”œâ”€â”€ .env.template                      # Template for credentials
â”œâ”€â”€ lakebase_credentials.conf.example  # Legacy config example
â”œâ”€â”€ lakebase_1m_benchmark.py          # Benchmark compatibility wrapper
â”œâ”€â”€ async_database_wrapper.py          # Async database compatibility wrapper
â”‚
â”œâ”€â”€ src/                               # Main source code
â”‚   â”œâ”€â”€ core/                          # Core functionality
â”‚   â”‚   â”œâ”€â”€ auth.py                    # Authentication management
â”‚   â”‚   â”œâ”€â”€ config.py                  # Configuration management
â”‚   â”‚   â”œâ”€â”€ pool.py                    # Connection pooling
â”‚   â”‚   â”œâ”€â”€ utils.py                   # Utility functions
â”‚   â”‚   â””â”€â”€ secure_credentials.py      # Secure credential management
â”‚   â”œâ”€â”€ monitoring/                    # Monitoring and health
â”‚   â”‚   â”œâ”€â”€ performance.py             # Performance monitoring
â”‚   â”‚   â”œâ”€â”€ health.py                  # Health monitoring
â”‚   â”‚   â”œâ”€â”€ error.py                   # Error handling
â”‚   â”‚   â””â”€â”€ fastapi_monitoring.py      # FastAPI monitoring
â”‚   â””â”€â”€ database/                      # Database connections
â”‚       â””â”€â”€ async_database.py          # Async database module
â”‚
â”œâ”€â”€ scripts/                           # Executable scripts
â”‚   â”œâ”€â”€ benchmark.py                   # Main benchmark script
â”‚   â”œâ”€â”€ setup.py                       # Setup script
â”‚   â”œâ”€â”€ config_helper.py               # Configuration helper
â”‚   â”œâ”€â”€ update_token.py                # Token update script
â”‚   â””â”€â”€ setup_secure_env.py            # Secure environment setup
â”‚
â”œâ”€â”€ tests/                             # Test files
â”œâ”€â”€ examples/                          # Example implementations
â”œâ”€â”€ docs/                              # Documentation
â”œâ”€â”€ config/                            # Configuration examples
â””â”€â”€ data/                              # Data files
```

## ğŸ” Secure Credential Management

This project uses **environment variables** instead of plain JSON files for secure credential management.

### Setup Secure Environment

1. **Create environment template:**
   ```bash
   python scripts/setup_secure_env.py template
   ```

2. **Configure your credentials:**
   ```bash
   cp .env.template .env
   # Edit .env with your actual credentials
   ```

3. **Validate configuration:**
   ```bash
   python scripts/setup_secure_env.py validate
   ```

### Required Environment Variables

```bash
# Databricks Configuration
DATABRICKS_HOST=https://your-workspace.cloud.databricks.com
LAKEFUSION_DATABRICKS_DAPI=dapi_your_personal_access_token_here
DATABRICKS_WORKSPACE_ID=your_workspace_id_here
DATABRICKS_USERNAME=your.email@company.com

# Lakebase Database Configuration
DATABRICKS_DATABASE_INSTANCE=your-benchmark-instance
DATABRICKS_DATABASE_HOST=instance-your-id.database.cloud.databricks.com
DATABRICKS_DATABASE_PORT=5432
DATABRICKS_DATABASE_NAME=databricks_postgres

# Connection Pool Configuration
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=30
DB_POOL_TIMEOUT=30
DB_POOL_RECYCLE=3000

# Service Configuration
SERVICE_NAME=LakeFusionDBService
DB_TYPE=lakebase
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Set Up Secure Credentials

```bash
python scripts/setup_secure_env.py template
cp .env.template .env
# Edit .env with your credentials
python scripts/setup_secure_env.py validate
```

### 3. Run Benchmark

```bash
python lakebase_1m_benchmark.py
```

### 4. Use in FastAPI

```python
from fastapi import FastAPI, Depends
from async_database_wrapper import get_db, lifespan

app = FastAPI(lifespan=lifespan)

@app.get("/")
async def root(db = Depends(get_db)):
    return {"message": "Hello World"}
```

## ğŸ“Š Benchmarking

The benchmark script tests Lakebase performance with:

- **100M+ row datasets** for realistic testing
- **Multi-threaded concurrent queries** (20-300 threads)
- **Connection pooling** with OAuth rate limiting mitigation
- **Comprehensive metrics** (throughput, latency, error rates)

### Benchmark Configuration

```python
# Default benchmark settings
thread_counts = [20, 40, 80, 100]  # Concurrent connections
test_duration = 30                  # Seconds per test
table_rows = 100000000             # 100M rows
batch_size = 10000                 # Batch size for operations
```

### Running Benchmarks

```bash
# Run full benchmark suite
python lakebase_1m_benchmark.py

# Run specific test
python tests/test_mysql_vs_lakebase.py

# Run comprehensive tests
python tests/comprehensive_test.py
```

## ğŸ”§ Database Connection Features

### Async Database Module

- **Async SQLAlchemy** connections with `asyncpg`
- **Background token refresh** every 50 minutes
- **Connection pooling** with health monitoring
- **Error handling** and automatic recovery
- **FastAPI integration** with lifespan management

### Connection Pool Configuration

```python
pool_size = 20                    # Base pool size
max_overflow = 30                 # Maximum overflow connections
pool_timeout = 30                 # Pool checkout timeout
pool_recycle = 3000              # Pool recycle interval
```

## ğŸ§ª Testing

### Run All Tests

```bash
python tests/comprehensive_test.py
```

### Test Categories

- âœ… **Dependencies** - All required packages
- âœ… **Configuration** - Secure credential validation
- âœ… **Benchmark** - Performance testing
- âœ… **Async Database** - Connection management
- âœ… **Async Functionality** - Async operations

## ğŸ“ˆ Performance Features

### Connection Pooling

- **Pre-warmed connections** for faster startup
- **Health monitoring** with automatic cleanup
- **OAuth rate limiting** mitigation
- **Background maintenance** tasks

### Monitoring

- **Performance metrics** collection
- **Health checks** for database connections
- **Error tracking** and categorization
- **Real-time monitoring** capabilities

## ğŸ”’ Security

### Credential Management

- âœ… **Environment variables** instead of JSON files
- âœ… **No hardcoded secrets** in code
- âœ… **Secure token storage** and rotation
- âœ… **Non-sensitive logging** only

### Best Practices

- Never commit `.env` files to version control
- Use secure credential storage in production
- Rotate tokens regularly
- Monitor access logs

## ğŸ“‹ Requirements

```
pydantic==2.11.5
pydantic-settings==2.9.1
python-dotenv==1.1.0
databricks-sdk==0.56.0
psycopg2-binary==2.9.9
fastapi==0.115.12
uvicorn==0.34.3
asyncpg==0.30.0
sqlalchemy==2.0.41
sqlmodel>=0.0.24
aiomysql==0.2.0
PyJWT==2.8.0
```

## ğŸ¯ Usage Examples

### Basic Database Connection

```python
from async_database_wrapper import get_db, init_engine

# Initialize database
init_engine()

# Use in FastAPI
@app.get("/users")
async def get_users(db = Depends(get_db)):
    result = await db.execute("SELECT * FROM users LIMIT 10")
    return result.fetchall()
```

### Custom Benchmark

```python
from scripts.benchmark import Lakebase1MBenchmark

benchmark = Lakebase1MBenchmark()
benchmark.thread_counts = [10, 20, 50]  # Custom thread counts
benchmark.test_duration = 60            # 60 seconds per test
benchmark.run_full_benchmark()
```

### Secure Environment Setup

```python
from src.core.secure_credentials import setup_secure_environment, validate_secure_credentials

# Set up secure environment
setup_secure_environment()

# Validate credentials
if validate_secure_credentials():
    print("âœ… Credentials are valid")
```

## ğŸš¨ Troubleshooting

### Common Issues

1. **Missing Environment Variables**
   ```bash
   python scripts/setup_secure_env.py validate
   ```

2. **Import Errors**
   ```bash
   pip install -r requirements.txt
   ```

3. **Database Connection Issues**
   - Check your Databricks credentials
   - Verify network connectivity
   - Check token expiration

### Debug Mode

```bash
# Enable debug logging
export LOG_LEVEL=DEBUG
python lakebase_1m_benchmark.py
```

## ğŸ“„ License

This project is for internal use at Databricks.

## ğŸ¤ Contributing

1. Follow secure credential management practices
2. Add tests for new features
3. Update documentation for changes
4. Use async patterns for database operations

---

**Note**: This project uses secure credential management with environment variables. Never commit `.env` files or hardcoded secrets to version control.
